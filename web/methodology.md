# The Bug Hunter's Methodology by Jason Haddix

# Table of content
- [Finding Seed Domains](#finding-seed-domains)
  - Finding Acquisitions
  - ASN Enumeration
  - Reverse WHOIS
  - Ad/Analytics Relationship Mapping
  - Google Fu
- [Finding Subdomains](#finding-subdomains)
  - Subdomain enumeration
    - Linked and JS Discovery
    - Subdomain Scraping
    - Subdomain Bruteforce

# Finding Seed Domains

## Finding Acquisitions
- https://www.crunchbase.com/

## ASN Enumeration
- Manual enumeration via https://bgp.he.net/
- Discover seed domains - `amass intel --asn`
- Automated enumeration
  - ASNLookup (maxmind.com dataset)
  - metabigor (bgp.he.net, asnlookup.com) 

## Reverse WHOIS
- https://whoxy.com/
- DOMLink (CLI of whoxy.com)

## Ad/Analytics Relationship Mapping
- https://buildwith.com/
- *getrelationship.py* (CLI by M4ll0k)

```bash
echo "example.com" | python3 getrelationship.com
```

## Google Fu
- "Copyright Text" inurl:example.com
- "Terms of Service Text" inurl:example.com
- "Privacy Policy Text" inurl:example.com

# Subdomain Enumeration
- [Linked and JS Discovery](#linked-and-js-discovery)
- [Subdomain Scraping](#subdomain-scraping)
- [Subdomain Bruteforce](#subdomain-bruteforce)

## Linked and JS Discovery

### Linked Discovery using Burp Suite Pro
- Set a scope item
  - Check "Use advanced scope controls"
  - Enter a term instead of an absolute domain name
  - Host or ip range: <keyword>
  - Site map > Filter by request type > Show only in-scope items
- Crawl all in-scope targets
  - Scan type: Crawl
  - Scan confgiruation
    - Crawl strategy - fastest
    - Never stop crawling due to application errors
  - Resource Pool
    - Name: <name>
    - Maximum concurrent requests: <~50-100>

## Subdomain Scraping

## Subdomain Bruteforce
